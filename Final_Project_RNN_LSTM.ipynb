{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "130f6b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "dataset_path = 'hour.csv'  \n",
    "bike_data = pd.read_csv(dataset_path)\n",
    "\n",
    "# Converting 'dteday' to datetime and extract features\n",
    "bike_data['dteday'] = pd.to_datetime(bike_data['dteday'])\n",
    "bike_data['day'] = bike_data['dteday'].dt.day\n",
    "bike_data['month'] = bike_data['dteday'].dt.month\n",
    "bike_data['year'] = bike_data['dteday'].dt.year\n",
    "\n",
    "\n",
    "features = bike_data.drop(['instant', 'casual','cnt', 'registered', 'dteday'], axis=1)\n",
    "target = bike_data['cnt']\n",
    "\n",
    "# Normalization\n",
    "scaler = MinMaxScaler()\n",
    "features_normalized = scaler.fit_transform(features)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_normalized, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reshaping\n",
    "X_train_rnn = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test_rnn = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3aebe900",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "348/348 [==============================] - 1s 1ms/step - loss: 27130.4531 - val_loss: 20572.0254\n",
      "Epoch 2/100\n",
      "348/348 [==============================] - 0s 748us/step - loss: 20132.5645 - val_loss: 19711.6875\n",
      "Epoch 3/100\n",
      "348/348 [==============================] - 0s 739us/step - loss: 19330.0723 - val_loss: 19067.4961\n",
      "Epoch 4/100\n",
      "348/348 [==============================] - 0s 743us/step - loss: 18240.4375 - val_loss: 17600.8164\n",
      "Epoch 5/100\n",
      "348/348 [==============================] - 0s 794us/step - loss: 17174.6367 - val_loss: 17324.2656\n",
      "Epoch 6/100\n",
      "348/348 [==============================] - 0s 749us/step - loss: 16107.7178 - val_loss: 15797.5811\n",
      "Epoch 7/100\n",
      "348/348 [==============================] - 0s 740us/step - loss: 15373.3779 - val_loss: 15244.7295\n",
      "Epoch 8/100\n",
      "348/348 [==============================] - 0s 738us/step - loss: 14772.1445 - val_loss: 14587.0293\n",
      "Epoch 9/100\n",
      "348/348 [==============================] - 0s 732us/step - loss: 14252.3418 - val_loss: 14291.1611\n",
      "Epoch 10/100\n",
      "348/348 [==============================] - 0s 739us/step - loss: 13731.2236 - val_loss: 13751.6230\n",
      "Epoch 11/100\n",
      "348/348 [==============================] - 0s 741us/step - loss: 13172.8428 - val_loss: 13127.9365\n",
      "Epoch 12/100\n",
      "348/348 [==============================] - 0s 736us/step - loss: 12703.7666 - val_loss: 12760.7334\n",
      "Epoch 13/100\n",
      "348/348 [==============================] - 0s 738us/step - loss: 12252.3018 - val_loss: 12138.5430\n",
      "Epoch 14/100\n",
      "348/348 [==============================] - 0s 737us/step - loss: 11723.1143 - val_loss: 11744.2197\n",
      "Epoch 15/100\n",
      "348/348 [==============================] - 0s 740us/step - loss: 11435.9229 - val_loss: 11528.9072\n",
      "Epoch 16/100\n",
      "348/348 [==============================] - 0s 745us/step - loss: 11022.1787 - val_loss: 11137.7910\n",
      "Epoch 17/100\n",
      "348/348 [==============================] - 0s 744us/step - loss: 10875.6963 - val_loss: 11381.6006\n",
      "Epoch 18/100\n",
      "348/348 [==============================] - 0s 737us/step - loss: 10592.7061 - val_loss: 10542.4541\n",
      "Epoch 19/100\n",
      "348/348 [==============================] - 0s 740us/step - loss: 10314.0303 - val_loss: 10930.4131\n",
      "Epoch 20/100\n",
      "348/348 [==============================] - 0s 740us/step - loss: 10239.0967 - val_loss: 10131.2676\n",
      "Epoch 21/100\n",
      "348/348 [==============================] - 0s 745us/step - loss: 9862.8760 - val_loss: 10345.1338\n",
      "Epoch 22/100\n",
      "348/348 [==============================] - 0s 745us/step - loss: 9722.6035 - val_loss: 10434.0371\n",
      "Epoch 23/100\n",
      "348/348 [==============================] - 0s 744us/step - loss: 9368.4365 - val_loss: 9534.3135\n",
      "Epoch 24/100\n",
      "348/348 [==============================] - 0s 745us/step - loss: 9233.7607 - val_loss: 10196.8203\n",
      "Epoch 25/100\n",
      "348/348 [==============================] - 0s 748us/step - loss: 8992.2842 - val_loss: 9232.7061\n",
      "Epoch 26/100\n",
      "348/348 [==============================] - 0s 745us/step - loss: 8655.5146 - val_loss: 9131.2080\n",
      "Epoch 27/100\n",
      "348/348 [==============================] - 0s 740us/step - loss: 8446.2256 - val_loss: 8316.5703\n",
      "Epoch 28/100\n",
      "348/348 [==============================] - 0s 741us/step - loss: 7943.5698 - val_loss: 8101.0894\n",
      "Epoch 29/100\n",
      "348/348 [==============================] - 0s 744us/step - loss: 7324.0762 - val_loss: 8064.6616\n",
      "Epoch 30/100\n",
      "348/348 [==============================] - 0s 755us/step - loss: 6820.1890 - val_loss: 6835.9678\n",
      "Epoch 31/100\n",
      "348/348 [==============================] - 0s 741us/step - loss: 6555.6030 - val_loss: 7552.6982\n",
      "Epoch 32/100\n",
      "348/348 [==============================] - 0s 751us/step - loss: 6054.0557 - val_loss: 5976.5195\n",
      "Epoch 33/100\n",
      "348/348 [==============================] - 0s 746us/step - loss: 5743.3325 - val_loss: 5704.9199\n",
      "Epoch 34/100\n",
      "348/348 [==============================] - 0s 759us/step - loss: 5391.3389 - val_loss: 5382.6636\n",
      "Epoch 35/100\n",
      "348/348 [==============================] - 0s 746us/step - loss: 5259.9453 - val_loss: 5343.9263\n",
      "Epoch 36/100\n",
      "348/348 [==============================] - 0s 741us/step - loss: 4841.7402 - val_loss: 4881.9634\n",
      "Epoch 37/100\n",
      "348/348 [==============================] - 0s 741us/step - loss: 4850.7295 - val_loss: 4613.8623\n",
      "Epoch 38/100\n",
      "348/348 [==============================] - 0s 888us/step - loss: 4689.2451 - val_loss: 5331.1196\n",
      "Epoch 39/100\n",
      "348/348 [==============================] - 0s 792us/step - loss: 4377.0479 - val_loss: 5145.6597\n",
      "Epoch 40/100\n",
      "348/348 [==============================] - 0s 759us/step - loss: 4370.9688 - val_loss: 4566.0903\n",
      "Epoch 41/100\n",
      "348/348 [==============================] - 0s 735us/step - loss: 4063.4434 - val_loss: 4720.1699\n",
      "Epoch 42/100\n",
      "348/348 [==============================] - 0s 729us/step - loss: 4058.4736 - val_loss: 4429.0146\n",
      "Epoch 43/100\n",
      "348/348 [==============================] - 0s 728us/step - loss: 3869.7659 - val_loss: 4479.7573\n",
      "Epoch 44/100\n",
      "348/348 [==============================] - 0s 729us/step - loss: 3703.4897 - val_loss: 3904.3398\n",
      "Epoch 45/100\n",
      "348/348 [==============================] - 0s 731us/step - loss: 3678.1770 - val_loss: 3771.9519\n",
      "Epoch 46/100\n",
      "348/348 [==============================] - 0s 726us/step - loss: 3632.0208 - val_loss: 3436.5925\n",
      "Epoch 47/100\n",
      "348/348 [==============================] - 0s 730us/step - loss: 3318.3027 - val_loss: 3665.9131\n",
      "Epoch 48/100\n",
      "348/348 [==============================] - 0s 726us/step - loss: 3610.1687 - val_loss: 3835.3230\n",
      "Epoch 49/100\n",
      "348/348 [==============================] - 0s 729us/step - loss: 3423.5166 - val_loss: 4033.3176\n",
      "Epoch 50/100\n",
      "348/348 [==============================] - 0s 730us/step - loss: 3214.9016 - val_loss: 3243.0090\n",
      "Epoch 51/100\n",
      "348/348 [==============================] - 0s 727us/step - loss: 3312.4758 - val_loss: 3442.3599\n",
      "Epoch 52/100\n",
      "348/348 [==============================] - 0s 777us/step - loss: 3145.1975 - val_loss: 3153.5635\n",
      "Epoch 53/100\n",
      "348/348 [==============================] - 0s 751us/step - loss: 3090.3049 - val_loss: 3055.4512\n",
      "Epoch 54/100\n",
      "348/348 [==============================] - 0s 738us/step - loss: 2932.8735 - val_loss: 3324.0176\n",
      "Epoch 55/100\n",
      "348/348 [==============================] - 0s 741us/step - loss: 2967.9424 - val_loss: 3135.9590\n",
      "Epoch 56/100\n",
      "348/348 [==============================] - 0s 755us/step - loss: 2931.2668 - val_loss: 2862.6367\n",
      "Epoch 57/100\n",
      "348/348 [==============================] - 0s 743us/step - loss: 2878.2947 - val_loss: 3587.2197\n",
      "Epoch 58/100\n",
      "348/348 [==============================] - 0s 750us/step - loss: 3060.2776 - val_loss: 2828.6699\n",
      "Epoch 59/100\n",
      "348/348 [==============================] - 0s 738us/step - loss: 2804.0261 - val_loss: 3721.9370\n",
      "Epoch 60/100\n",
      "348/348 [==============================] - 0s 747us/step - loss: 2855.7676 - val_loss: 3025.3147\n",
      "Epoch 61/100\n",
      "348/348 [==============================] - 0s 734us/step - loss: 2780.6433 - val_loss: 3293.7192\n",
      "Epoch 62/100\n",
      "348/348 [==============================] - 0s 737us/step - loss: 2719.8887 - val_loss: 3227.7385\n",
      "Epoch 63/100\n",
      "348/348 [==============================] - 0s 738us/step - loss: 2749.3054 - val_loss: 2661.7620\n",
      "Epoch 64/100\n",
      "348/348 [==============================] - 0s 740us/step - loss: 2499.5786 - val_loss: 2876.1189\n",
      "Epoch 65/100\n",
      "348/348 [==============================] - 0s 749us/step - loss: 2691.5984 - val_loss: 6094.7026\n",
      "Epoch 66/100\n",
      "348/348 [==============================] - 0s 763us/step - loss: 2653.2705 - val_loss: 3179.5293\n",
      "Epoch 67/100\n",
      "348/348 [==============================] - 0s 781us/step - loss: 2514.7239 - val_loss: 2944.0515\n",
      "Epoch 68/100\n",
      "348/348 [==============================] - 0s 739us/step - loss: 2498.0774 - val_loss: 2718.3599\n",
      "Epoch 69/100\n",
      "348/348 [==============================] - 0s 741us/step - loss: 2567.4734 - val_loss: 3071.4980\n",
      "Epoch 70/100\n",
      "348/348 [==============================] - 0s 731us/step - loss: 2353.0415 - val_loss: 2417.6824\n",
      "Epoch 71/100\n",
      "348/348 [==============================] - 0s 735us/step - loss: 2744.3599 - val_loss: 2532.3960\n",
      "Epoch 72/100\n",
      "348/348 [==============================] - 0s 743us/step - loss: 2525.3499 - val_loss: 2931.2539\n",
      "Epoch 73/100\n",
      "348/348 [==============================] - 0s 750us/step - loss: 2344.1750 - val_loss: 2784.4080\n",
      "Epoch 74/100\n",
      "348/348 [==============================] - 0s 748us/step - loss: 2387.9294 - val_loss: 3008.5378\n",
      "Epoch 75/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "348/348 [==============================] - 0s 735us/step - loss: 2245.6121 - val_loss: 2294.2249\n",
      "Epoch 76/100\n",
      "348/348 [==============================] - 0s 713us/step - loss: 2260.8757 - val_loss: 2597.5764\n",
      "Epoch 77/100\n",
      "348/348 [==============================] - 0s 723us/step - loss: 2391.0952 - val_loss: 2698.1501\n",
      "Epoch 78/100\n",
      "348/348 [==============================] - 0s 716us/step - loss: 2218.7771 - val_loss: 2203.6841\n",
      "Epoch 79/100\n",
      "348/348 [==============================] - 0s 711us/step - loss: 2346.5095 - val_loss: 3055.2046\n",
      "Epoch 80/100\n",
      "348/348 [==============================] - 0s 716us/step - loss: 2239.0779 - val_loss: 2351.2290\n",
      "Epoch 81/100\n",
      "348/348 [==============================] - 0s 721us/step - loss: 2193.3130 - val_loss: 2218.9016\n",
      "Epoch 82/100\n",
      "348/348 [==============================] - 0s 717us/step - loss: 2253.9426 - val_loss: 2779.0242\n",
      "Epoch 83/100\n",
      "348/348 [==============================] - 0s 717us/step - loss: 2171.0332 - val_loss: 2517.8691\n",
      "Epoch 84/100\n",
      "348/348 [==============================] - 0s 719us/step - loss: 2271.8022 - val_loss: 3136.9194\n",
      "Epoch 85/100\n",
      "348/348 [==============================] - 0s 725us/step - loss: 2289.3901 - val_loss: 2156.6096\n",
      "Epoch 86/100\n",
      "348/348 [==============================] - 0s 721us/step - loss: 2199.2336 - val_loss: 2321.9443\n",
      "Epoch 87/100\n",
      "348/348 [==============================] - 0s 715us/step - loss: 2039.8696 - val_loss: 2601.1948\n",
      "Epoch 88/100\n",
      "348/348 [==============================] - 0s 728us/step - loss: 2073.8708 - val_loss: 2471.0698\n",
      "Epoch 89/100\n",
      "348/348 [==============================] - 0s 727us/step - loss: 2120.0303 - val_loss: 2260.3926\n",
      "Epoch 90/100\n",
      "348/348 [==============================] - 0s 731us/step - loss: 2113.3088 - val_loss: 2299.9951\n",
      "Epoch 91/100\n",
      "348/348 [==============================] - 0s 726us/step - loss: 2017.8937 - val_loss: 2761.4019\n",
      "Epoch 92/100\n",
      "348/348 [==============================] - 0s 735us/step - loss: 2394.4800 - val_loss: 2177.4102\n",
      "Epoch 93/100\n",
      "348/348 [==============================] - 0s 727us/step - loss: 1977.0084 - val_loss: 2419.9375\n",
      "Epoch 94/100\n",
      "348/348 [==============================] - 0s 735us/step - loss: 1977.0565 - val_loss: 2367.4302\n",
      "Epoch 95/100\n",
      "348/348 [==============================] - 0s 732us/step - loss: 2070.4590 - val_loss: 2215.7815\n",
      "Epoch 96/100\n",
      "348/348 [==============================] - 0s 711us/step - loss: 2090.6377 - val_loss: 2416.9673\n",
      "Epoch 97/100\n",
      "348/348 [==============================] - 0s 727us/step - loss: 2172.3882 - val_loss: 2233.7781\n",
      "Epoch 98/100\n",
      "348/348 [==============================] - 0s 725us/step - loss: 1963.3124 - val_loss: 2550.2451\n",
      "Epoch 99/100\n",
      "348/348 [==============================] - 0s 722us/step - loss: 2272.4133 - val_loss: 2358.0210\n",
      "Epoch 100/100\n",
      "348/348 [==============================] - 0s 738us/step - loss: 1972.6853 - val_loss: 2368.1396\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x333797280>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "#RNN model\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(60, activation='relu', return_sequences=True, input_shape=(X_train_rnn.shape[1], X_train_rnn.shape[2])))\n",
    "model.add(SimpleRNN(60, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.005), loss='mean_squared_error')\n",
    "\n",
    "# Training\n",
    "model.fit(X_train_rnn, y_train, epochs=100, batch_size=32, validation_split=0.2, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5c0248b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109/109 [==============================] - 0s 373us/step\n",
      "RMSE: 47.15638507552373\n",
      "R2 Score: 0.9297743323223191\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y_pred = model.predict(X_test_rnn)\n",
    "\n",
    "#Performance\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f'RMSE: {rmse}')\n",
    "print(f'R2 Score: {r2}')\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "90b9bbd5",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "from hyperopt import hp, tpe, Trials, fmin\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Define the space of hyperparameters to search\n",
    "space = {\n",
    "    'units': hp.choice('units', [50, 100, 150]),\n",
    "    'activation': hp.choice('activation', ['relu', 'tanh']),\n",
    "    'learning_rate': hp.loguniform('learning_rate', np.log(0.001), np.log(0.1)),\n",
    "    'batch_size': hp.choice('batch_size', [32, 64, 128]),\n",
    "}\n",
    "\n",
    "# Objective function to minimize\n",
    "def objective(params):\n",
    "    model = Sequential()\n",
    "    model.add(SimpleRNN(params['units'], activation=params['activation'], return_sequences=True, input_shape=(X_train_rnn.shape[1], X_train_rnn.shape[2])))\n",
    "    model.add(SimpleRNN(params['units'], activation=params['activation']))\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=params['learning_rate']), loss='mean_squared_error')\n",
    "\n",
    "    model.fit(X_train_rnn, y_train, epochs=100, batch_size=params['batch_size'], verbose=0)\n",
    "    \n",
    "    y_pred = model.predict(X_test_rnn)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    # We aim to maximize R2 score, hence minimize negative of R2 score\n",
    "    return -r2\n",
    "\n",
    "# Run the algorithm\n",
    "best = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=10,  # You can adjust this number\n",
    "            trials=Trials())\n",
    "\n",
    "print(\"Best: \", best)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a323cd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3c295ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(50, activation='relu', return_sequences=True, input_shape=(X_train_rnn.shape[1], X_train_rnn.shape[2])))\n",
    "model.add(LSTM(50, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.005), loss='mean_squared_error')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ee35c42f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "348/348 [==============================] - 1s 1ms/step - loss: 28839.0195 - val_loss: 20567.8984\n",
      "Epoch 2/100\n",
      "348/348 [==============================] - 0s 980us/step - loss: 20150.6484 - val_loss: 19796.8789\n",
      "Epoch 3/100\n",
      "348/348 [==============================] - 0s 978us/step - loss: 18616.7637 - val_loss: 17077.1055\n",
      "Epoch 4/100\n",
      "348/348 [==============================] - 0s 983us/step - loss: 16139.5488 - val_loss: 15229.1406\n",
      "Epoch 5/100\n",
      "348/348 [==============================] - 0s 959us/step - loss: 14941.0322 - val_loss: 14682.6191\n",
      "Epoch 6/100\n",
      "348/348 [==============================] - 0s 969us/step - loss: 14286.8467 - val_loss: 14113.5508\n",
      "Epoch 7/100\n",
      "348/348 [==============================] - 0s 955us/step - loss: 13849.6064 - val_loss: 13467.7383\n",
      "Epoch 8/100\n",
      "348/348 [==============================] - 0s 960us/step - loss: 13005.4150 - val_loss: 12852.4648\n",
      "Epoch 9/100\n",
      "348/348 [==============================] - 0s 960us/step - loss: 12241.4453 - val_loss: 11902.7354\n",
      "Epoch 10/100\n",
      "348/348 [==============================] - 0s 963us/step - loss: 11605.6318 - val_loss: 11692.0996\n",
      "Epoch 11/100\n",
      "348/348 [==============================] - 0s 955us/step - loss: 11131.0830 - val_loss: 11087.1885\n",
      "Epoch 12/100\n",
      "348/348 [==============================] - 0s 956us/step - loss: 11004.7090 - val_loss: 11000.6201\n",
      "Epoch 13/100\n",
      "348/348 [==============================] - 0s 959us/step - loss: 10587.1367 - val_loss: 10557.1367\n",
      "Epoch 14/100\n",
      "348/348 [==============================] - 0s 965us/step - loss: 8744.1104 - val_loss: 8338.0000\n",
      "Epoch 15/100\n",
      "348/348 [==============================] - 0s 955us/step - loss: 7535.9980 - val_loss: 6943.0742\n",
      "Epoch 16/100\n",
      "348/348 [==============================] - 0s 950us/step - loss: 6886.3867 - val_loss: 7397.5454\n",
      "Epoch 17/100\n",
      "348/348 [==============================] - 0s 961us/step - loss: 6284.0444 - val_loss: 5862.1465\n",
      "Epoch 18/100\n",
      "348/348 [==============================] - 0s 961us/step - loss: 5735.1367 - val_loss: 5275.3086\n",
      "Epoch 19/100\n",
      "348/348 [==============================] - 0s 976us/step - loss: 5533.6367 - val_loss: 4818.4424\n",
      "Epoch 20/100\n",
      "348/348 [==============================] - 0s 970us/step - loss: 5275.8071 - val_loss: 4623.7109\n",
      "Epoch 21/100\n",
      "348/348 [==============================] - 0s 953us/step - loss: 4628.5049 - val_loss: 4329.7715\n",
      "Epoch 22/100\n",
      "348/348 [==============================] - 0s 960us/step - loss: 4334.0059 - val_loss: 4081.6089\n",
      "Epoch 23/100\n",
      "348/348 [==============================] - 0s 960us/step - loss: 4167.8818 - val_loss: 4315.6836\n",
      "Epoch 24/100\n",
      "348/348 [==============================] - 0s 973us/step - loss: 3673.0012 - val_loss: 3737.9148\n",
      "Epoch 25/100\n",
      "348/348 [==============================] - 0s 1ms/step - loss: 3813.3069 - val_loss: 4012.1931\n",
      "Epoch 26/100\n",
      "348/348 [==============================] - 0s 977us/step - loss: 3677.7854 - val_loss: 3501.0911\n",
      "Epoch 27/100\n",
      "348/348 [==============================] - 0s 977us/step - loss: 3574.3962 - val_loss: 3398.0127\n",
      "Epoch 28/100\n",
      "348/348 [==============================] - 0s 980us/step - loss: 3549.0466 - val_loss: 3799.4036\n",
      "Epoch 29/100\n",
      "348/348 [==============================] - 0s 985us/step - loss: 3436.8652 - val_loss: 3360.2722\n",
      "Epoch 30/100\n",
      "348/348 [==============================] - 0s 1ms/step - loss: 3324.9314 - val_loss: 3962.0173\n",
      "Epoch 31/100\n",
      "348/348 [==============================] - 0s 1ms/step - loss: 3141.5144 - val_loss: 2983.1558\n",
      "Epoch 32/100\n",
      "348/348 [==============================] - 0s 977us/step - loss: 3155.1912 - val_loss: 3045.3611\n",
      "Epoch 33/100\n",
      "348/348 [==============================] - 0s 983us/step - loss: 3164.0332 - val_loss: 3746.7261\n",
      "Epoch 34/100\n",
      "348/348 [==============================] - 0s 981us/step - loss: 3136.3506 - val_loss: 4075.4700\n",
      "Epoch 35/100\n",
      "348/348 [==============================] - 0s 978us/step - loss: 3182.2896 - val_loss: 3076.9939\n",
      "Epoch 36/100\n",
      "348/348 [==============================] - 0s 973us/step - loss: 2892.2495 - val_loss: 2891.3848\n",
      "Epoch 37/100\n",
      "348/348 [==============================] - 0s 984us/step - loss: 2936.0120 - val_loss: 3506.9211\n",
      "Epoch 38/100\n",
      "348/348 [==============================] - 0s 983us/step - loss: 2900.5032 - val_loss: 3087.3884\n",
      "Epoch 39/100\n",
      "348/348 [==============================] - 0s 981us/step - loss: 2855.3765 - val_loss: 2894.7273\n",
      "Epoch 40/100\n",
      "348/348 [==============================] - 0s 987us/step - loss: 2754.9888 - val_loss: 4164.5654\n",
      "Epoch 41/100\n",
      "348/348 [==============================] - 0s 984us/step - loss: 2852.2263 - val_loss: 3001.8401\n",
      "Epoch 42/100\n",
      "348/348 [==============================] - 0s 988us/step - loss: 2893.1660 - val_loss: 2794.6001\n",
      "Epoch 43/100\n",
      "348/348 [==============================] - 0s 987us/step - loss: 2629.0044 - val_loss: 3167.3567\n",
      "Epoch 44/100\n",
      "348/348 [==============================] - 0s 979us/step - loss: 2785.0034 - val_loss: 3179.7188\n",
      "Epoch 45/100\n",
      "348/348 [==============================] - 0s 973us/step - loss: 2631.9348 - val_loss: 2919.8879\n",
      "Epoch 46/100\n",
      "348/348 [==============================] - 0s 971us/step - loss: 2608.0198 - val_loss: 3416.0166\n",
      "Epoch 47/100\n",
      "348/348 [==============================] - 0s 968us/step - loss: 2626.7375 - val_loss: 2854.5967\n",
      "Epoch 48/100\n",
      "348/348 [==============================] - 0s 1ms/step - loss: 2569.4148 - val_loss: 2630.8057\n",
      "Epoch 49/100\n",
      "348/348 [==============================] - 0s 969us/step - loss: 2526.6460 - val_loss: 2722.2488\n",
      "Epoch 50/100\n",
      "348/348 [==============================] - 0s 974us/step - loss: 2505.5256 - val_loss: 2704.6289\n",
      "Epoch 51/100\n",
      "348/348 [==============================] - 0s 975us/step - loss: 2615.0059 - val_loss: 2952.7458\n",
      "Epoch 52/100\n",
      "348/348 [==============================] - 0s 972us/step - loss: 2434.3943 - val_loss: 2697.9268\n",
      "Epoch 53/100\n",
      "348/348 [==============================] - 0s 969us/step - loss: 2449.9424 - val_loss: 2826.7781\n",
      "Epoch 54/100\n",
      "348/348 [==============================] - 0s 967us/step - loss: 2485.2544 - val_loss: 2423.3618\n",
      "Epoch 55/100\n",
      "348/348 [==============================] - 0s 968us/step - loss: 2391.2644 - val_loss: 2440.8784\n",
      "Epoch 56/100\n",
      "348/348 [==============================] - 0s 971us/step - loss: 2264.6472 - val_loss: 2692.3892\n",
      "Epoch 57/100\n",
      "348/348 [==============================] - 0s 966us/step - loss: 2378.8379 - val_loss: 2546.5339\n",
      "Epoch 58/100\n",
      "348/348 [==============================] - 0s 953us/step - loss: 2221.3596 - val_loss: 3222.0420\n",
      "Epoch 59/100\n",
      "348/348 [==============================] - 0s 974us/step - loss: 2325.0684 - val_loss: 2450.2979\n",
      "Epoch 60/100\n",
      "348/348 [==============================] - 0s 965us/step - loss: 2275.0369 - val_loss: 2392.1316\n",
      "Epoch 61/100\n",
      "348/348 [==============================] - 0s 972us/step - loss: 2266.8018 - val_loss: 2340.6003\n",
      "Epoch 62/100\n",
      "348/348 [==============================] - 0s 978us/step - loss: 2208.9316 - val_loss: 2352.0410\n",
      "Epoch 63/100\n",
      "348/348 [==============================] - 0s 975us/step - loss: 2235.6699 - val_loss: 2569.5032\n",
      "Epoch 64/100\n",
      "348/348 [==============================] - 0s 967us/step - loss: 2193.7522 - val_loss: 2580.1074\n",
      "Epoch 65/100\n",
      "348/348 [==============================] - 0s 997us/step - loss: 2156.2556 - val_loss: 2280.9526\n",
      "Epoch 66/100\n",
      "348/348 [==============================] - 0s 975us/step - loss: 2266.0474 - val_loss: 2532.5603\n",
      "Epoch 67/100\n",
      "348/348 [==============================] - 0s 982us/step - loss: 2141.2227 - val_loss: 2244.7500\n",
      "Epoch 68/100\n",
      "348/348 [==============================] - 0s 978us/step - loss: 2000.3744 - val_loss: 2372.6462\n",
      "Epoch 69/100\n",
      "348/348 [==============================] - 0s 982us/step - loss: 2191.6296 - val_loss: 3034.0173\n",
      "Epoch 70/100\n",
      "348/348 [==============================] - 0s 977us/step - loss: 2151.1990 - val_loss: 2569.7439\n",
      "Epoch 71/100\n",
      "348/348 [==============================] - 0s 979us/step - loss: 2048.3586 - val_loss: 2456.1570\n",
      "Epoch 72/100\n",
      "348/348 [==============================] - 0s 979us/step - loss: 2242.6602 - val_loss: 2443.3091\n",
      "Epoch 73/100\n",
      "348/348 [==============================] - 0s 981us/step - loss: 1991.7123 - val_loss: 2439.7759\n",
      "Epoch 74/100\n",
      "348/348 [==============================] - 0s 966us/step - loss: 2027.3706 - val_loss: 2391.7417\n",
      "Epoch 75/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "348/348 [==============================] - 0s 962us/step - loss: 2011.3395 - val_loss: 2213.7869\n",
      "Epoch 76/100\n",
      "348/348 [==============================] - 0s 983us/step - loss: 1981.3138 - val_loss: 2350.3438\n",
      "Epoch 77/100\n",
      "348/348 [==============================] - 0s 1ms/step - loss: 2047.1418 - val_loss: 2482.5740\n",
      "Epoch 78/100\n",
      "348/348 [==============================] - 0s 961us/step - loss: 1965.3066 - val_loss: 2443.4150\n",
      "Epoch 79/100\n",
      "348/348 [==============================] - 0s 952us/step - loss: 1938.7020 - val_loss: 2949.1189\n",
      "Epoch 80/100\n",
      "348/348 [==============================] - 0s 954us/step - loss: 1998.3854 - val_loss: 2865.5701\n",
      "Epoch 81/100\n",
      "348/348 [==============================] - 0s 955us/step - loss: 1907.6221 - val_loss: 2121.0532\n",
      "Epoch 82/100\n",
      "348/348 [==============================] - 0s 961us/step - loss: 2016.2037 - val_loss: 2783.6418\n",
      "Epoch 83/100\n",
      "348/348 [==============================] - 0s 956us/step - loss: 1958.4481 - val_loss: 2264.8105\n",
      "Epoch 84/100\n",
      "348/348 [==============================] - 0s 950us/step - loss: 1882.9647 - val_loss: 2290.4089\n",
      "Epoch 85/100\n",
      "348/348 [==============================] - 0s 959us/step - loss: 1877.5643 - val_loss: 2291.6465\n",
      "Epoch 86/100\n",
      "348/348 [==============================] - 0s 951us/step - loss: 2012.1638 - val_loss: 2170.8816\n",
      "Epoch 87/100\n",
      "348/348 [==============================] - 0s 946us/step - loss: 2030.8555 - val_loss: 4222.8843\n",
      "Epoch 88/100\n",
      "348/348 [==============================] - 0s 958us/step - loss: 1906.0626 - val_loss: 2147.4971\n",
      "Epoch 89/100\n",
      "348/348 [==============================] - 0s 956us/step - loss: 1879.5062 - val_loss: 2965.5583\n",
      "Epoch 90/100\n",
      "348/348 [==============================] - 0s 966us/step - loss: 1841.1255 - val_loss: 2081.8169\n",
      "Epoch 91/100\n",
      "348/348 [==============================] - 0s 971us/step - loss: 1859.3671 - val_loss: 2068.6108\n",
      "Epoch 92/100\n",
      "348/348 [==============================] - 0s 954us/step - loss: 1902.6310 - val_loss: 2538.0190\n",
      "Epoch 93/100\n",
      "348/348 [==============================] - 0s 956us/step - loss: 1857.7977 - val_loss: 2323.3721\n",
      "Epoch 94/100\n",
      "348/348 [==============================] - 0s 964us/step - loss: 1837.4767 - val_loss: 2277.7490\n",
      "Epoch 95/100\n",
      "348/348 [==============================] - 0s 963us/step - loss: 1790.3345 - val_loss: 2145.2756\n",
      "Epoch 96/100\n",
      "348/348 [==============================] - 0s 964us/step - loss: 1891.8230 - val_loss: 2470.6367\n",
      "Epoch 97/100\n",
      "348/348 [==============================] - 0s 957us/step - loss: 2220.3354 - val_loss: 2173.2310\n",
      "Epoch 98/100\n",
      "348/348 [==============================] - 0s 966us/step - loss: 1736.3877 - val_loss: 2124.7888\n",
      "Epoch 99/100\n",
      "348/348 [==============================] - 0s 951us/step - loss: 1773.2224 - val_loss: 2069.3379\n",
      "Epoch 100/100\n",
      "348/348 [==============================] - 0s 963us/step - loss: 1870.6722 - val_loss: 2072.4341\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x31b841940>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_rnn, y_train, epochs=100, batch_size=32, validation_split=0.2, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b0a1b312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109/109 [==============================] - 0s 361us/step\n",
      "RMSE: 43.438677148302624\n",
      "R2 Score: 0.9404107323178197\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "y_pred = model.predict(X_test_rnn)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f'RMSE: {rmse}')\n",
    "print(f'R2 Score: {r2}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7ee884",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (yourenvname)",
   "language": "python",
   "name": "yourenvname"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
